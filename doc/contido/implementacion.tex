\chapter{Implementación}
\label{chap:implementacion}

\lettrine{I}{mplementación} es la ejecución o puesta en marcha de una idea programada. Y sobre ello versa este capítulo: la documentación de la puesta en marcha de lo planeado en el capítulo anterior. Primero se ha de crear el soporte hardware, para posterioremente desplegar el software sobre este mismo.

Como se comenta posteriormente en (TODO \#REF\# metodología), se sigue un modelo basado en prototipos, en el que se realiza un codiseño hardware y software mediante el uso de máquinas virtuales.

\section{Configuración Hardware}
\label{sec:configuracion_hardware}
En esta sección se documenta el transcurso del montaje y puesta a punto del hardware del cluster, los errores cometidos, y se muestran fotografías del proceso.

Como se comenta en \nameref{ssec:diseño_estructural}, el objetivo es construir un cluster compacto, y para ello las diferentes partes se ensamblan por separado. Podemos distinguir así cinco fases:
\begin{itemize}
    \item \textbf{Montaje} de las \textbf{torres} de Raspberry Pi: Se debe montar cada piso de la torre, acomodando cada Raspberry en cada nivel (es decir, colocando disipadores, anclajes, y conectores magnéticos USB) como se muestra en la figura \ref{fig:modulo_raspi_torre}, donde se ve un módulo de la torre. Asimismo, en la figura \ref{fig:proceso_apilamiento} se puede ver el proceso de apilamiento.

    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{img/modulo_raspi_torre.jpg}
    \caption{Un módulo de la torre completamente ensamblado}
    \label{fig:modulo_raspi_torre}
    \end{figure}

    \begin{figure}[h!]
    \centering
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/apilamiento/2.jpg}
        \caption{Dos Raspberry Pis apiladas}
        \label{fig:apilamiento_2}
    \end{subfigure}
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/apilamiento/3.jpg}
        \caption{Tres}
        \label{fig:apilamiento_3}
    \end{subfigure}

    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/apilamiento/4.jpg}
        \caption{Primera torre completa}
        \label{fig:apilamiento_4}
    \end{subfigure}
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/apilamiento/8.jpg}
        \caption{Y ambas torres}
        \label{fig:apilamiento_8}
    \end{subfigure}
    \caption{Proceso de apilamiento de las torres de Raspberry Pis}
    \label{fig:proceso_apilamiento}
    \end{figure}
    
    \item \textbf{Testeo} del correcto funcionamiento de cada \textbf{unidad}: En esta fase se realizan las pruebas básicas para comprobar que todas las placas y componentes se han recibido conforme a los estándares de calidad necesarios y funcionan correctamente. Este test ligero y muy superficial consiste en el arranque del sistema operativo Arch Linux en cada mini computador, testeo de las interfaces de red y USB (figura \ref{fig:testeo_hw_1}), así como el circuito de entrega de potencia mediante un pequeño test de estrés. Finalmente se actualiza el bootloader a la última versión (figura \ref{fig:testeo_hw_2}).

    \begin{figure}[h!]
    \centering
    \begin{subfigure}[c]{0.8\textwidth}
        \includegraphics[width=\textwidth]{img/testeo_hw/infraestructura_testeo.jpg}
        \caption{Infraestructura mientras se realiza el testeo de una \acrshort{rpi}}
        \label{fig:testeo_hw_1}
    \end{subfigure}

    \begin{subfigure}[c]{0.8\textwidth}
        \includegraphics[width=\textwidth]{img/testeo_hw/update_BL.jpg}
        \caption{Actualización del Bootloader de una \acrshort{rpi}}
        \label{fig:testeo_hw_2}
    \end{subfigure}
    \caption{Proceso de testeo y puesta a punto de los componentes del cluster}
    \label{fig:testeo_hw}
    \end{figure}

    \item \textbf{Cortado} de las \textbf{láminas adaptadoras} de chapa: Ya con las piezas testeadas llega el momento de ensamblar todo el sistema. Para ello se cortan con radial planchas de chapa recicladas de una caja antigua de ordenador, se lijan, pintan, y se realizan los agujeros necesarios para acoplar cada capa con la siguiente. En este aspecto, si bien estéticamente podría haber sido mejorable en caso de haber dispuesto de una plegadora, y dado que este proyecto no es de ingeniería industrial, se considera adecuada la solución construída.


    \item \textbf{Ensamblado final} y cableado: Se integran todos los componentes en la estructura, y se realiza el cableado, tanto eléctrico como de red. Este cableado es cortado a medida para un ajuste fino, libre de excedentes que gestionar. (Figura \ref{fig:fotos_estructura})

    \begin{figure}[h!]
    \centering
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/fotos_estructura/front.jpg}
        \caption{Parte frontal}
        \label{fig:fotos_estructura_front}
    \end{subfigure}
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/fotos_estructura/back.jpg}
        \caption{Parte trasera}
        \label{fig:fotos_estructura_back}
    \end{subfigure}

    \begin{subfigure}[c]{0.7\textwidth}
        \includegraphics[width=\textwidth]{img/fotos_estructura/style.jpg}
        \caption{Foto de perfil}
        \label{fig:fotos_estructura_style}
    \end{subfigure}
    \caption{Estructura de Clupiter terminada}
    \label{fig:fotos_estructura}
    \end{figure}

    \item \textbf{Testeo} de la correcta \textbf{integración}: Finalmente se debe comprobar que todos los componentes continúan funcionando correctamente una vez instalados. Este paso trajo ciertas complicaciones, ya que durante el montaje se desprendieron pequeñas limaduras de hierro, que si bien se limpiaron a conciencia de las placas y todo circuito eléctrico, los conectores magnéticos USB-C recogieron todo ese residuo, y como se comenta en \nameref{sssec:cableado_electrico_red}, esto dió algunos problemas con malos contactos y caídas de tensión intermitentes y aleatorias. En el proceso de la solución de errores se desmontó de nuevo el cluster para realizar otra limpieza e inspección de cableado, llegando así a la versión actual.
\end{itemize}

\section{Configuración Software}
\label{sec:configuracion_software}
A continuación se documentan los pasos realizados para llevar los nodos del cluster al estado actual. Esta documentación asume cierto conocimiento de comandos de Linux.

\subsection{Instalación Base del Sistema Operativo}
\label{ssec:instalacion_sistema_operativo}
Para instalar Arch Linux en las Raspberry Pi 4, se debe acudir a la página web de ArchLinuxARM\footnote{\url{https://archlinuxarm.org/platforms/armv8/broadcom/raspberry-pi-4}} y seguir las instrucciones. La instalación, que consiste en la copia del sistema a la microSD, se debe realizar desde un ordenador con Linux. En este caso, al no tener dependencias fuertes en las librerías de 32 bits, se ha optado por la versión de 64 bits, que otorgará considerablemente mayor rendimiento.\footnote{\url{https://matteocroce.medium.com/bd5290d48947}}

Así, se comienza por descargar el sistema de archivos raíz\footnote{\texttt{man hier} para más detalle} (\textit{\gls{rootfs}}) desde la web:

\begin{lstlisting}[language=bash,basicstyle=\scriptsize]
wget http://os.archlinuxarm.org/os/ArchLinuxARM-rpi-aarch64-latest.tar.gz
\end{lstlisting}

A continuación, y asumiendo que se conectarán en \texttt{/dev/sdd} las microSD donde se flasheará Arch Linux, se ejecutará el siguiente script para cada una de las tarjetas, el cual las particiona, da formato, y extrae el sistema de archivos sobre ellas, permitiendo así el arranque en las Raspberry Pis:

\begin{lstlisting}[language=bash]
#!/bin/sh

MICROSD=/dev/sdd

echo "Copiando Arch Linux sobre ${MICROSD}"

sed -e 's/\s*\([\+0-9a-zA-Z]*\).*/\1/' << EOF | fdisk ${MICROSD}
    o       # Se limpia la tabla de particiones
    n       # Se crea una nueva partición
    p       #  primaria
    1       #  número uno
            #  por defecto al principio del disco
    +200M   #  y con un tamaño de 200MiB
    t       # Se cambia el tipo de la partición
    c       #  a W95 FAT32 (LBA)
    n       # Se crea otra partición
    p       #  primaria
    2       #  número dos
            #  a continuación de la primera
            #  y que ocupe todo el disco
    w       # Se escribe la tabla de particiones
EOF

# Se crean carpetas para montar las particiones 1 (boot) y 2 (root)
mkdir -p boot root

# Y se formatean los sistemas de ficheros
mkfs.vfat ${MICROSD}1
mkfs.ext4 ${MICROSD}2

# Se montan las particiones
mount ${MICROSD}1 boot
mount ${MICROSD}2 root

# Y se extrae el tarball a root
bsdtar -xpf ArchLinuxARM-rpi-aarch64-latest.tar.gz -C root

# Se mueve el directorio boot a su partición
mv root/boot/* boot

# Se realiza una modificación en el fstab para adaptarlo a 64 bits
sed -i 's/mmcblk0/mmcblk1/g' root/etc/fstab

# Se escriben los datos a la SD y se desmontan las carpetas
sync
umount boot root
\end{lstlisting}

Tras ello se conecta el cluster a la red ya existente (por ejemplo, desconectando una Raspberry del switch y conectando un cable al puerto vacío) y se buscarán las Raspberry Pis con el comando: (nótese que el comando puede variar en función del servidor DHCP, y por tanto, debe colocarse la dirección de red adecuada)

\begin{lstlisting}[language=bash]
nmap -sP -PR 192.168.0.*
\end{lstlisting}

De esta forma y tras encontrar los dispositivos, se puede hacer ssh a cada uno de ellos, configurarlos, actualizarlos, y, en definitiva, ponerlos a punto.\footnote{Contraseñas \texttt{alarm:alarm}, \texttt{root:root}}

Como dato curioso, y que puede servir como regla mnemotécnica, comentar que el nombre y contraseña del usuario no privilegiado: \texttt{alarm} es una composición de las iniciales \textbf{A}rch \textbf{L}inux \textbf{ARM}. En cuanto a \texttt{root}, este nombre es el habitual del usuario con UID 0 en sistemas *NIX (aunque puede no ser el único).

Para inicializar los sistemas en las \acrshort{rpi}s se procede con los siguientes comandos:

\begin{lstlisting}[language=bash]
ssh alarm@<IP>
su -                # Se loguea como root

# Se inicializa y puebla el keyring para el gestor de paquetes
pacman-key --init
pacman-key --populate archlinuxarm

# Y se actualiza el sistema a la última versión
pacman -Syyu

# Se cambia el hostname del sistema
#  Se sigue el esquema de nombres rpiX, siendo la 0 y 1
#   - rpi0
#   - rpi1
echo rpiX > /etc/hostname

# Esto es un ajuste algo más personal, pero que mejora la experiencia y velocidad en las actualizaciones: se edita el archivo /etc/pacman.conf
#  Se descomenta
#   Color
#   ParallelDownloads (y se deja el valor predeterminado de 5)
nano /etc/pacman.conf

# Tras esto el sistema debe reiniciarse para cargar el kernel (más que probablemente) actualizado. Tras el arranque se debe verificar que todo funcione correctamente.
reboot
\end{lstlisting}

\subsection{Bridging de Interfaces}
Como el switch tiene el mismo número de puertos que el número de \acrshort{rpi}s, como ya se ha comprobado en el apartado anterior, dada la necesidad de realizar la inicialización de las placas en dos pasos, se necesita una forma de conectar el cluster a un ordenador o red externa. Para ello se puentea el adaptador USB 3.0 a Gigabit Ethernet que se podrá conectar en cualquier puerto USB del nodo maestro (rpi0).

Las configuraciones a realizar en \texttt{/etc/systemd/network} son las siguientes:

\begin{itemize}
    \item Para los archivos \texttt{en.network} y \texttt{eth.network}, desactivar DHCP comentando las dos líneas a continuación, y activar la pertenencia al bridge maestro en \texttt{br0}:
\begin{lstlisting}[language=bash]
...

[Network]
Bridge=br0
#DHCP=yes
#DNSSEC=no
\end{lstlisting}
    \item Crear el network device \texttt{br0.netdev}
\begin{lstlisting}[language=bash]
[NetDev]
Name=br0
Kind=bridge
\end{lstlisting}
    \item Y asignar una IP al bridge en \texttt{br0.network}
\begin{lstlisting}[language=bash]
[Match]
Name=br0

[Network]
DHCP=yes
DNSSEC=no
\end{lstlisting}
\end{itemize}

Tras realizar estas configuraciones se reinicia \texttt{systemd-networkd} con
\begin{lstlisting}[language=bash]
systemctl restart systemd-networkd
\end{lstlisting}

Por último se deberá reconectar la sesión por SSH, ya que al haber cambiado la MAC de la interfaz\footnote{systemd-networkd assigns a MAC address generated based on the interface name and the machine ID to the bridge. This may cause connection issues, for example in case of routing based on MAC filtering. To circumvent such problems you may assign a MAC address to your bridge, probably the same as your physical device, adding the line \texttt{MACAddress=xx:xx:xx:xx:xx:xx} in the NetDev section above. \cite{archwiki_systemd-networkd}}, salvo en algún extraño caso, el servidor DHCP en uso le habrá cambiado también la IP asignada a \texttt{rpi0}.

\subsection{Configuración de direcciones IP estáticas}
\label{ssec:configuracion_ip_estaticas}
A la hora de administrar el cluster es necesario que las direcciones se asignen de alguna forma, sea esta estática o dinámica. Sin embargo, debido a las características del cluster, y especialmente a que la conexión desde el exterior va a ser variable y no siempre va a haber internet, lo más conveniente es configurar direcciones IP estáticas en cada uno de los hosts que componen el cluster, así como una dirección de gateway predeterminada, que algunas veces estará up, y otras no.

Para esto, se han de modificar los archivos ubicados en \texttt{/etc/systemd/network/} como se indica a continuación:

\begin{itemize}
    \item Modificar \texttt{eth.network} en las máquinas 1 a 7, y asignarles una IP estática, siendo la X en \texttt{.22X} el número de la máquina, así \texttt{rpi1} sería \texttt{.221}, etc.
\begin{lstlisting}[language=bash]
[Match]
Name=eth*

[Network]
Address=192.168.0.22X/24
Gateway=192.168.0.1
DNS=8.8.8.8
#DHCP=yes
#DNSSEC=no
\end{lstlisting}
    \item Proceder forma similar con el nodo maestro, asignando una IP estática al bridge en \texttt{br0.network}:
\begin{lstlisting}[language=bash]
[Match]
Name=br0

[Network]
Address=192.168.0.220/24
Gateway=192.168.0.1
DNS=8.8.8.8
#DHCP=yes
#DNSSEC=no
\end{lstlisting}
\end{itemize}

\subsection{Escritura del fichero /etc/hosts}
Para hacer referencia al resto de \acrshort{rpi}s sin necesidad de escribir su dirección IP (lo que será útil más adelante para la ejecución de programas \acrshort{mpi}), es conveniente agregar una entrada por dispositivo a cada una de ellas en el fichero \texttt{/etc/hosts} para poder referenciarlas por su nombre, sin necesidad de configurar un servidor DNS propio.

De esta manera, se ejecuta en cada nodo:
\begin{lstlisting}[language=bash]
cat << EOS >> /etc/hosts
127.0.0.1       localhost
192.168.0.220   rpi0
192.168.0.221   rpi1
192.168.0.222   rpi2
192.168.0.223   rpi3
192.168.0.224   rpi4
192.168.0.225   rpi5
192.168.0.226   rpi6
192.168.0.227   rpi7
EOS
\end{lstlisting}

\subsection{Creación del usuario mpiuser}
Hasta este punto, cada dispositivo cuenta con dos usuarios, \texttt{root} y \texttt{alarm}, ninguno de los cuales es apto para lanzar los programas \acrshort{mpi}.

El uso de la cuenta de superusuario no es recomendable para tareas diferentes a las estrictamente necesarias, y la cuenta no privilegiada \texttt{alarm} es la que se utiliza como usuario por defecto no privilegiado, por lo que es conveniente crear un tercero que se encargue del lanzamiento de los programas \acrshort{mpi}, y así definir las responsabilidades y permisos de cada usuario adecuadamente. Este nuevo usuario tendrá por nombre \texttt{mpiuser}. Para ello se ejecuta en todos los nodos:

\begin{lstlisting}[language=bash]
# Crear el usuario mpiuser y su carpeta en /home/mpiuser
useradd -m mpiuser
# Y asignar la contraseña 'mpi'}.
echo "mpiuser:mpi" | chpasswd
\end{lstlisting}

\subsection{Generación de claves \acrshort{ssh}}
\label{ssec:gen_ssh}
Tras la creación de los usuarios para la ejecución de programas paralelos con \acrshort{mpi}, es necesario configurar la autenticación no interactiva para \acrshort{ssh}. Esto se realiza mediante el uso de autenticación mediante par de claves pública y privada.

Sabiendo ya la tarea a realizar, ésta se puede llevar a cabo de forma muy sencilla ejecutando en todos los nodos del cluster como usuario \texttt{mpiuser} (\texttt{su - mpiuser}) el siguiente comando. Es importante no establecer \textit{passphrase}, ya que si no se perdería la no-interactividad.
\begin{lstlisting}[language=bash]
ssh-keygen
\end{lstlisting}

Como será el nodo maestro el que lance las tareas, se debe copiar la clave pública del nodo maestro al resto de nodos, lo cual se puede semi-automatizar con el siguiente comando (ya que aún se deberá escribir la contraseña):

\begin{lstlisting}[language=bash]
for i in {1..7}; do
    ssh-copy-id mpiuser@rpi$i
done
\end{lstlisting}

Y tras ello se comprobará que se han copiado satisfactoriamente a todos los nodos con:
\begin{lstlisting}[language=bash]
for i in {1..7}; do
    ssh mpiuser@rpi$i "cat /etc/hostname"
done
\end{lstlisting}
Este último comando debería mostrar los hostnames del resto de nodos del cluster, sin interrumpir al usuario preguntando contraseña, es decir, en modo no-interactivo.


\subsection{Servidor NFS}
\label{ssec:serv_nfs}
Debido a que las llamadas a \texttt{mpirun} con múltiples hosts ejecutan el mismo comando en todos ellos, debe existir algún tipo de almacenamiento compartido montado con el mismo nombre en todos los nodos. Para satisfacer, esta necesidad de almacenamiento compartido, la elección es NFS.

El servidor será el nodo maestro, y el resto serán clientes. Los pasos a realizar en todos los nodos (como root) son los siguientes:

\begin{lstlisting}[language=bash]
# Instalar el paquete de NFS en todos los nodos
pacman -S nfs-utils

# Se crea la carpeta /mpishared y se le dan permisos
mkdir -p /mpishared
chown mpiuser:mpiuser /mpishared
\end{lstlisting}

A continuación se ejecutan los siguientes comandos \textbf{únicamente} en el nodo maestro:

\begin{lstlisting}[language=bash]
# Añadir la carpeta compartida a exports
echo "/mpishared 192.168.0.0/24(ro,sync,fsid=0)" >> /etc/exports

# Y se recargan los exports
exportfs -arv

# Tras ello se activa e inicia el servidor nfs
systemctl enable --now nfs-server.service
\end{lstlisting}

Por otro lado se añade al fstab del resto de nodos (\textbf{1-7}) la siguiente línea:
\begin{lstlisting}[language=bash]
echo "rpi0:/ /mpishared nfs defaults,timeo=900,retrans=5,"\
"x-systemd.requires=network-online.target,_netdev 0 0" >> /etc/fstab
\end{lstlisting}

\subsubsection{Solución de problemas}
Podría parecer que todo está correcto, pero hay un sutil problema en esta configuración: Dependiendo si rpi0 se apaga primero o no, el resto de nodos pueden quedarse esperando a que responda el share de NFS. Solucionar esto es más sencillo de lo que podría parecer: se ha de establecer un pequeño timeout antes de detener el servidor NFS de, por ejemplo, 5 segundos.

\begin{lstlisting}[language=bash]
# Extender la unit del servidor nfs
systemctl edit nfs-server.service

# Se añade en la zona editable
[Service]
ExecStop=
ExecStop=/bin/sleep 5
ExecStop=/usr/sbin/rpc.nfsd 0

# Salir y ejecutar
systemctl daemon-reload
\end{lstlisting}